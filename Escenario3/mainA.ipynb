{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcFDCnAFCBpT"
      },
      "source": [
        "# Implementación Red Neuronal con CUDA\n",
        "## Luis Miguel Henao y Mariana López"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljhjdD3_CUbA"
      },
      "source": [
        "### Importación de librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7gu_sN0B52s",
        "outputId": "41b1a9aa-8a36-473d-8090-588d45c97afc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing mainA.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile mainA.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "#include <time.h>\n",
        "#include <string.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// MACRO DE SEGURIDAD PARA CUDA\n",
        "// Si alguna función de CUDA falla, esto imprime el error y la línea exacta.\n",
        "#define CUDA_CHECK(call) \\\n",
        "    do { \\\n",
        "        cudaError_t err = call; \\\n",
        "        if (err != cudaSuccess) { \\\n",
        "            fprintf(stderr, \"Error CUDA en %s:%d: %s\\n\", __FILE__, __LINE__, cudaGetErrorString(err)); \\\n",
        "            exit(1); \\\n",
        "        } \\\n",
        "    } while (0)\n",
        "\n",
        "#pragma region Clases y Estructuras\n",
        "\n",
        "typedef struct\n",
        "{\n",
        "    int filas;\n",
        "    int columnas;\n",
        "    float *datos;\n",
        "} Matriz;\n",
        "\n",
        "Matriz *crear_matriz(int filas, int columnas)\n",
        "{\n",
        "    Matriz *m = (Matriz *)malloc(sizeof(Matriz));\n",
        "    m->filas = filas;\n",
        "    m->columnas = columnas;\n",
        "    m->datos = (float *)malloc(filas * columnas * sizeof(float));\n",
        "    return m;\n",
        "}\n",
        "\n",
        "#pragma endregion\n",
        "\n",
        "#pragma region KERNEL CUDA\n",
        "\n",
        "__global__ void matmul_kernel(float *A, float *B, float *C, int rowsA, int colsA, int colsB)\n",
        "{\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < rowsA && col < colsB)\n",
        "    {\n",
        "        float sum = 0.0f;\n",
        "        for (int k = 0; k < colsA; k++)\n",
        "        {\n",
        "            sum += A[row * colsA + k] * B[k * colsB + col];\n",
        "        }\n",
        "        C[row * colsB + col] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "#pragma endregion\n",
        "\n",
        "#pragma region Manipular Matrices\n",
        "\n",
        "void liberar_matriz(Matriz *m)\n",
        "{\n",
        "    if (m)\n",
        "    {\n",
        "        if (m->datos) free(m->datos);\n",
        "        free(m);\n",
        "    }\n",
        "}\n",
        "\n",
        "// *** CORRECCIÓN CRÍTICA AQUÍ ***\n",
        "void inicializar_matriz_aleatoria(Matriz *m)\n",
        "{\n",
        "    // Escalar por 0.1 es VITAL para evitar \"Dying ReLU\" y saturación inicial\n",
        "    float escala = 0.1f;\n",
        "    for (int i = 0; i < m->filas * m->columnas; i++)\n",
        "    {\n",
        "        m->datos[i] = (((float)rand() / RAND_MAX) - 0.5f) * escala;\n",
        "    }\n",
        "}\n",
        "\n",
        "void limpiar_matriz(Matriz *m)\n",
        "{\n",
        "    memset(m->datos, 0, m->filas * m->columnas * sizeof(float));\n",
        "}\n",
        "\n",
        "// Función robusta con manejo de errores y memoria GPU\n",
        "void multiplicar_matrices(Matriz *A, Matriz *B, Matriz *C)\n",
        "{\n",
        "    if (A->columnas != B->filas)\n",
        "    {\n",
        "        printf(\"Error Dimensiones: %dx%d * %dx%d\\n\", A->filas, A->columnas, B->filas, B->columnas);\n",
        "        exit(1);\n",
        "    }\n",
        "\n",
        "    size_t size_A = A->filas * A->columnas * sizeof(float);\n",
        "    size_t size_B = B->filas * B->columnas * sizeof(float);\n",
        "    size_t size_C = C->filas * C->columnas * sizeof(float);\n",
        "\n",
        "    float *d_A, *d_B, *d_C;\n",
        "\n",
        "    // Reservar memoria en GPU con chequeo de errores\n",
        "    CUDA_CHECK(cudaMalloc((void**)&d_A, size_A));\n",
        "    CUDA_CHECK(cudaMalloc((void**)&d_B, size_B));\n",
        "    CUDA_CHECK(cudaMalloc((void**)&d_C, size_C));\n",
        "\n",
        "    // Copiar a GPU\n",
        "    CUDA_CHECK(cudaMemcpy(d_A, A->datos, size_A, cudaMemcpyHostToDevice));\n",
        "    CUDA_CHECK(cudaMemcpy(d_B, B->datos, size_B, cudaMemcpyHostToDevice));\n",
        "\n",
        "    // Configuración del Kernel\n",
        "    dim3 threadsPerBlock(16, 16);\n",
        "    dim3 blocksPerGrid((C->columnas + 15) / 16, (C->filas + 15) / 16);\n",
        "\n",
        "    // Lanzar Kernel\n",
        "    matmul_kernel<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, A->filas, A->columnas, B->columnas);\n",
        "\n",
        "    // Chequear si el lanzamiento falló\n",
        "    CUDA_CHECK(cudaGetLastError());\n",
        "\n",
        "    // Sincronizar\n",
        "    CUDA_CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "    // Traer resultados\n",
        "    CUDA_CHECK(cudaMemcpy(C->datos, d_C, size_C, cudaMemcpyDeviceToHost));\n",
        "\n",
        "    // Liberar GPU\n",
        "    cudaFree(d_A);\n",
        "    cudaFree(d_B);\n",
        "    cudaFree(d_C);\n",
        "}\n",
        "\n",
        "void transpuesta(Matriz *A, Matriz *B)\n",
        "{\n",
        "    for (int i = 0; i < A->filas; i++)\n",
        "        for (int j = 0; j < A->columnas; j++)\n",
        "            B->datos[j * B->columnas + i] = A->datos[i * A->columnas + j];\n",
        "}\n",
        "\n",
        "void sumar_sesgo(Matriz *m, Matriz *b)\n",
        "{\n",
        "    for (int i = 0; i < m->filas; i++)\n",
        "        for (int j = 0; j < m->columnas; j++)\n",
        "            m->datos[i * m->columnas + j] += b->datos[j];\n",
        "}\n",
        "\n",
        "void multiplicar_escalar(Matriz *m, float escalar)\n",
        "{\n",
        "    for (int i = 0; i < m->filas * m->columnas; i++)\n",
        "        m->datos[i] *= escalar;\n",
        "}\n",
        "\n",
        "void restar_matrices(Matriz *A, Matriz *B)\n",
        "{\n",
        "    for (int i = 0; i < A->filas * A->columnas; i++)\n",
        "        A->datos[i] -= B->datos[i];\n",
        "}\n",
        "\n",
        "#pragma endregion\n",
        "\n",
        "#pragma region Funciones Auxiliares\n",
        "\n",
        "void relu(Matriz *m)\n",
        "{\n",
        "    for (int i = 0; i < m->filas * m->columnas; i++)\n",
        "        if (m->datos[i] < 0) m->datos[i] = 0;\n",
        "}\n",
        "\n",
        "void softmax(Matriz *m)\n",
        "{\n",
        "    for (int i = 0; i < m->filas; i++)\n",
        "    {\n",
        "        float max_val = -1e9;\n",
        "        for (int j = 0; j < m->columnas; j++)\n",
        "            if (m->datos[i * m->columnas + j] > max_val) max_val = m->datos[i * m->columnas + j];\n",
        "\n",
        "        float sum = 0.0f;\n",
        "        for (int j = 0; j < m->columnas; j++)\n",
        "        {\n",
        "            m->datos[i * m->columnas + j] = expf(m->datos[i * m->columnas + j] - max_val);\n",
        "            sum += m->datos[i * m->columnas + j];\n",
        "        }\n",
        "        for (int j = 0; j < m->columnas; j++)\n",
        "            m->datos[i * m->columnas + j] /= sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "int argmax(Matriz *m, int row)\n",
        "{\n",
        "    float max_val = -1e9;\n",
        "    int max_index = 0;\n",
        "    for (int j = 0; j < m->columnas; j++)\n",
        "    {\n",
        "        if (m->datos[row * m->columnas + j] > max_val)\n",
        "        {\n",
        "            max_val = m->datos[row * m->columnas + j];\n",
        "            max_index = j;\n",
        "        }\n",
        "    }\n",
        "    return max_index;\n",
        "}\n",
        "\n",
        "float calcular_precision(Matriz *predicciones, Matriz *etiquetas)\n",
        "{\n",
        "    int correctos = 0;\n",
        "    for (int i = 0; i < predicciones->filas; i++)\n",
        "    {\n",
        "        if (argmax(predicciones, i) == (int)etiquetas->datos[i])\n",
        "            correctos++;\n",
        "    }\n",
        "    return (float)correctos / predicciones->filas;\n",
        "}\n",
        "\n",
        "float calcular_loss(Matriz *predicciones, Matriz *etiquetas, int num_clases)\n",
        "{\n",
        "    float loss = 0.0f;\n",
        "    const float epsilon = 1e-12f;\n",
        "    for (int i = 0; i < predicciones->filas; i++)\n",
        "    {\n",
        "        int etiqueta = (int)etiquetas->datos[i];\n",
        "        float pred = predicciones->datos[i * num_clases + etiqueta];\n",
        "        pred = fmaxf(pred, epsilon);\n",
        "        loss += -logf(pred);\n",
        "    }\n",
        "    return loss / predicciones->filas;\n",
        "}\n",
        "\n",
        "// Función para imprimir memoria de GPU\n",
        "void imprimir_uso_gpu() {\n",
        "    size_t free_byte ;\n",
        "    size_t total_byte ;\n",
        "    cudaError_t cuda_status = cudaMemGetInfo( &free_byte, &total_byte ) ;\n",
        "\n",
        "    if ( cudaSuccess != cuda_status ){\n",
        "        printf(\"Error: cudaMemGetInfo falla %s \\n\", cudaGetErrorString(cuda_status) );\n",
        "        return ;\n",
        "    }\n",
        "\n",
        "    double free_db = (double)free_byte ;\n",
        "    double total_db = (double)total_byte ;\n",
        "    double used_db = total_db - free_db ;\n",
        "\n",
        "    printf(\"  [GPU Mem] Usada: %.2f MB | Libre: %.2f MB | Total: %.2f MB\\n\",\n",
        "        used_db/1024.0/1024.0, free_db/1024.0/1024.0, total_db/1024.0/1024.0);\n",
        "}\n",
        "\n",
        "#pragma endregion\n",
        "\n",
        "#pragma region Cargar Datos\n",
        "\n",
        "int convertir_bytes_a_enteros(FILE *fp)\n",
        "{\n",
        "    unsigned char buf[4];\n",
        "    if (fread(buf, sizeof(unsigned char), 4, fp) != 4) return 0;\n",
        "    return (buf[0] << 24) | (buf[1] << 16) | (buf[2] << 8) | buf[3];\n",
        "}\n",
        "\n",
        "Matriz *cargar_imagenes_dataset(const char *filename)\n",
        "{\n",
        "    FILE *fp = fopen(filename, \"rb\");\n",
        "    if (!fp) { printf(\"Error abriendo %s\\n\", filename); exit(1); }\n",
        "\n",
        "    convertir_bytes_a_enteros(fp);\n",
        "    int num_imgs = convertir_bytes_a_enteros(fp);\n",
        "    int rows = convertir_bytes_a_enteros(fp);\n",
        "    int cols = convertir_bytes_a_enteros(fp);\n",
        "\n",
        "    Matriz *m = crear_matriz(num_imgs, rows * cols);\n",
        "    unsigned char temp;\n",
        "    for (int i = 0; i < m->filas * m->columnas; i++)\n",
        "    {\n",
        "        if (fread(&temp, sizeof(unsigned char), 1, fp) != 1) break;\n",
        "        m->datos[i] = (float)temp / 255.0f;\n",
        "    }\n",
        "    fclose(fp);\n",
        "    return m;\n",
        "}\n",
        "\n",
        "Matriz *cargar_etiquetas_dataset(const char *filename)\n",
        "{\n",
        "    FILE *fp = fopen(filename, \"rb\");\n",
        "    if (!fp) { printf(\"Error abriendo %s\\n\", filename); exit(1); }\n",
        "\n",
        "    convertir_bytes_a_enteros(fp);\n",
        "    int num_items = convertir_bytes_a_enteros(fp);\n",
        "\n",
        "    Matriz *m = crear_matriz(num_items, 1);\n",
        "    unsigned char temp;\n",
        "    for (int i = 0; i < num_items; i++)\n",
        "    {\n",
        "        if (fread(&temp, sizeof(unsigned char), 1, fp) != 1) break;\n",
        "        m->datos[i] = (float)temp;\n",
        "    }\n",
        "    fclose(fp);\n",
        "    return m;\n",
        "}\n",
        "\n",
        "#pragma endregion\n",
        "\n",
        "int main()\n",
        "{\n",
        "    srand(time(NULL));\n",
        "\n",
        "    const int TAMAÑO_ENTRADA = 784;\n",
        "    const int TAMAÑO_CAPA_OCULTA = 512;\n",
        "    const int TAMAÑO_SALIDA = 10;\n",
        "    const float TASA_APRENDIZAJE = 0.01f;\n",
        "    const int EPOCAS = 10;\n",
        "    const int TAMAÑO_BATCH = 512;\n",
        "\n",
        "    printf(\"=== MLP en CUDA (Optimizado) ===\\n\");\n",
        "\n",
        "    // Verificar GPU inicial\n",
        "    imprimir_uso_gpu();\n",
        "\n",
        "    printf(\"\\nCargando datos...\\n\");\n",
        "    Matriz *X_train = cargar_imagenes_dataset(\"./Resources/train-images.idx3-ubyte\");\n",
        "    Matriz *Y_train = cargar_etiquetas_dataset(\"./Resources/train-labels.idx1-ubyte\");\n",
        "    Matriz *X_test = cargar_imagenes_dataset(\"./Resources/t10k-images.idx3-ubyte\");\n",
        "    Matriz *Y_test = cargar_etiquetas_dataset(\"./Resources/t10k-labels.idx1-ubyte\");\n",
        "\n",
        "    // Verificación rápida de datos\n",
        "    float suma_check = 0;\n",
        "    for(int i=0; i<784; i++) suma_check += X_train->datos[i];\n",
        "    if(suma_check < 1.0f) { printf(\"ERROR: Datos parecen vacíos.\\n\"); return 1; }\n",
        "    printf(\"Datos cargados correctamente. Train: %d, Test: %d\\n\", X_train->filas, X_test->filas);\n",
        "\n",
        "    Matriz *W1 = crear_matriz(TAMAÑO_ENTRADA, TAMAÑO_CAPA_OCULTA);\n",
        "    inicializar_matriz_aleatoria(W1);\n",
        "    Matriz *b1 = crear_matriz(1, TAMAÑO_CAPA_OCULTA);\n",
        "    limpiar_matriz(b1);\n",
        "    Matriz *W2 = crear_matriz(TAMAÑO_CAPA_OCULTA, TAMAÑO_SALIDA);\n",
        "    inicializar_matriz_aleatoria(W2);\n",
        "    Matriz *b2 = crear_matriz(1, TAMAÑO_SALIDA);\n",
        "    limpiar_matriz(b2);\n",
        "\n",
        "    // Buffers reutilizables\n",
        "    Matriz *X_batch = crear_matriz(TAMAÑO_BATCH, TAMAÑO_ENTRADA);\n",
        "    Matriz *Y_batch = crear_matriz(TAMAÑO_BATCH, 1);\n",
        "    Matriz *Z1 = crear_matriz(TAMAÑO_BATCH, TAMAÑO_CAPA_OCULTA);\n",
        "    Matriz *A1 = crear_matriz(TAMAÑO_BATCH, TAMAÑO_CAPA_OCULTA);\n",
        "    Matriz *Z2 = crear_matriz(TAMAÑO_BATCH, TAMAÑO_SALIDA);\n",
        "    Matriz *A2 = crear_matriz(TAMAÑO_BATCH, TAMAÑO_SALIDA);\n",
        "    Matriz *dZ2 = crear_matriz(TAMAÑO_BATCH, TAMAÑO_SALIDA);\n",
        "    Matriz *dW2 = crear_matriz(TAMAÑO_CAPA_OCULTA, TAMAÑO_SALIDA);\n",
        "    Matriz *db2 = crear_matriz(1, TAMAÑO_SALIDA);\n",
        "    Matriz *A1_T = crear_matriz(TAMAÑO_CAPA_OCULTA, TAMAÑO_BATCH);\n",
        "    Matriz *dZ1 = crear_matriz(TAMAÑO_BATCH, TAMAÑO_CAPA_OCULTA);\n",
        "    Matriz *dW1 = crear_matriz(TAMAÑO_ENTRADA, TAMAÑO_CAPA_OCULTA);\n",
        "    Matriz *db1 = crear_matriz(1, TAMAÑO_CAPA_OCULTA);\n",
        "    Matriz *W2_T = crear_matriz(TAMAÑO_SALIDA, TAMAÑO_CAPA_OCULTA);\n",
        "    Matriz *X_batch_T = crear_matriz(TAMAÑO_ENTRADA, TAMAÑO_BATCH);\n",
        "\n",
        "    int *indices = (int *)malloc(X_train->filas * sizeof(int));\n",
        "    for (int k = 0; k < X_train->filas; k++) indices[k] = k;\n",
        "\n",
        "    clock_t inicio_total = clock();\n",
        "\n",
        "    printf(\"\\nIniciando Entrenamiento...\\n\");\n",
        "    for (int epoca = 0; epoca < EPOCAS; epoca++)\n",
        "    {\n",
        "        clock_t inicio_epoca = clock(); // Cronómetro época\n",
        "\n",
        "        // Shuffle\n",
        "        for (int k = X_train->filas - 1; k > 0; k--)\n",
        "        {\n",
        "            int j = rand() % (k + 1);\n",
        "            int temp = indices[k]; indices[k] = indices[j]; indices[j] = temp;\n",
        "        }\n",
        "\n",
        "        for (int i = 0; i < X_train->filas; i += TAMAÑO_BATCH)\n",
        "        {\n",
        "            int batch_actual = (i + TAMAÑO_BATCH > X_train->filas) ? X_train->filas - i : TAMAÑO_BATCH;\n",
        "\n",
        "            // Cargar Batch\n",
        "            for (int b = 0; b < batch_actual; b++)\n",
        "            {\n",
        "                int idx = indices[i + b];\n",
        "                memcpy(&X_batch->datos[b * TAMAÑO_ENTRADA], &X_train->datos[idx * TAMAÑO_ENTRADA], TAMAÑO_ENTRADA * sizeof(float));\n",
        "                Y_batch->datos[b] = Y_train->datos[idx];\n",
        "            }\n",
        "\n",
        "            // Ajustar dimensiones lógicas\n",
        "            X_batch->filas = batch_actual; Z1->filas = batch_actual; A1->filas = batch_actual;\n",
        "            Z2->filas = batch_actual; A2->filas = batch_actual;\n",
        "\n",
        "            // Forward\n",
        "            multiplicar_matrices(X_batch, W1, Z1);\n",
        "            sumar_sesgo(Z1, b1);\n",
        "            memcpy(A1->datos, Z1->datos, batch_actual * TAMAÑO_CAPA_OCULTA * sizeof(float));\n",
        "            relu(A1);\n",
        "\n",
        "            multiplicar_matrices(A1, W2, Z2);\n",
        "            sumar_sesgo(Z2, b2);\n",
        "            memcpy(A2->datos, Z2->datos, batch_actual * TAMAÑO_SALIDA * sizeof(float));\n",
        "            softmax(A2);\n",
        "\n",
        "            // Backward\n",
        "            memcpy(dZ2->datos, A2->datos, batch_actual * TAMAÑO_SALIDA * sizeof(float));\n",
        "            for (int b = 0; b < batch_actual; b++)\n",
        "                dZ2->datos[b * TAMAÑO_SALIDA + (int)Y_batch->datos[b]] -= 1.0f;\n",
        "\n",
        "            A1->filas = batch_actual;\n",
        "            transpuesta(A1, A1_T);\n",
        "            multiplicar_matrices(A1_T, dZ2, dW2);\n",
        "            multiplicar_escalar(dW2, 1.0f / batch_actual);\n",
        "\n",
        "            limpiar_matriz(db2);\n",
        "            for (int r = 0; r < batch_actual; r++)\n",
        "                for (int c = 0; c < TAMAÑO_SALIDA; c++)\n",
        "                    db2->datos[c] += dZ2->datos[r * TAMAÑO_SALIDA + c];\n",
        "            multiplicar_escalar(db2, 1.0f / batch_actual);\n",
        "\n",
        "            transpuesta(W2, W2_T);\n",
        "            multiplicar_matrices(dZ2, W2_T, dZ1);\n",
        "            for (int k = 0; k < batch_actual * TAMAÑO_CAPA_OCULTA; k++)\n",
        "                if (Z1->datos[k] <= 0) dZ1->datos[k] = 0.0f;\n",
        "\n",
        "            X_batch->filas = batch_actual;\n",
        "            transpuesta(X_batch, X_batch_T);\n",
        "            multiplicar_matrices(X_batch_T, dZ1, dW1);\n",
        "            multiplicar_escalar(dW1, 1.0f / batch_actual);\n",
        "\n",
        "            limpiar_matriz(db1);\n",
        "            for (int r = 0; r < batch_actual; r++)\n",
        "                for (int c = 0; c < TAMAÑO_CAPA_OCULTA; c++)\n",
        "                    db1->datos[c] += dZ1->datos[r * TAMAÑO_CAPA_OCULTA + c];\n",
        "            multiplicar_escalar(db1, 1.0f / batch_actual);\n",
        "\n",
        "            multiplicar_escalar(dW1, TASA_APRENDIZAJE); restar_matrices(W1, dW1);\n",
        "            multiplicar_escalar(db1, TASA_APRENDIZAJE); restar_matrices(b1, db1);\n",
        "            multiplicar_escalar(dW2, TASA_APRENDIZAJE); restar_matrices(W2, dW2);\n",
        "            multiplicar_escalar(db2, TASA_APRENDIZAJE); restar_matrices(b2, db2);\n",
        "        }\n",
        "\n",
        "        // Métricas de fin de época\n",
        "        double tiempo_epoca = (double)(clock() - inicio_epoca) / CLOCKS_PER_SEC;\n",
        "\n",
        "        Matriz *Z1_eval = crear_matriz(X_train->filas, TAMAÑO_CAPA_OCULTA);\n",
        "        Matriz *A1_eval = crear_matriz(X_train->filas, TAMAÑO_CAPA_OCULTA);\n",
        "        Matriz *Z2_eval = crear_matriz(X_train->filas, TAMAÑO_SALIDA);\n",
        "        Matriz *A2_eval = crear_matriz(X_train->filas, TAMAÑO_SALIDA);\n",
        "\n",
        "        multiplicar_matrices(X_train, W1, Z1_eval);\n",
        "        sumar_sesgo(Z1_eval, b1);\n",
        "        memcpy(A1_eval->datos, Z1_eval->datos, X_train->filas * TAMAÑO_CAPA_OCULTA * sizeof(float));\n",
        "        relu(A1_eval);\n",
        "        multiplicar_matrices(A1_eval, W2, Z2_eval);\n",
        "        sumar_sesgo(Z2_eval, b2);\n",
        "        memcpy(A2_eval->datos, Z2_eval->datos, X_train->filas * TAMAÑO_SALIDA * sizeof(float));\n",
        "        softmax(A2_eval);\n",
        "\n",
        "        float acc = calcular_precision(A2_eval, Y_train);\n",
        "        float loss = calcular_loss(A2_eval, Y_train, TAMAÑO_SALIDA);\n",
        "\n",
        "        printf(\"Epoca %d/%d | Tiempo: %.2fs | Loss: %.4f | Acc: %.4f\",\n",
        "               epoca + 1, EPOCAS, tiempo_epoca, loss, acc);\n",
        "\n",
        "        // Imprimir uso de GPU al final de la línea\n",
        "        imprimir_uso_gpu();\n",
        "\n",
        "        liberar_matriz(Z1_eval); liberar_matriz(A1_eval);\n",
        "        liberar_matriz(Z2_eval); liberar_matriz(A2_eval);\n",
        "    }\n",
        "\n",
        "    double total_time = (double)(clock() - inicio_total) / CLOCKS_PER_SEC;\n",
        "    printf(\"\\nEntrenamiento finalizado en %.2f segundos.\\n\", total_time);\n",
        "\n",
        "    // Evaluación Final\n",
        "    Matriz *Z1_test = crear_matriz(X_test->filas, TAMAÑO_CAPA_OCULTA);\n",
        "    Matriz *A1_test = crear_matriz(X_test->filas, TAMAÑO_CAPA_OCULTA);\n",
        "    Matriz *Z2_test = crear_matriz(X_test->filas, TAMAÑO_SALIDA);\n",
        "    Matriz *A2_test = crear_matriz(X_test->filas, TAMAÑO_SALIDA);\n",
        "\n",
        "    multiplicar_matrices(X_test, W1, Z1_test);\n",
        "    sumar_sesgo(Z1_test, b1);\n",
        "    memcpy(A1_test->datos, Z1_test->datos, X_test->filas * TAMAÑO_CAPA_OCULTA * sizeof(float));\n",
        "    relu(A1_test);\n",
        "    multiplicar_matrices(A1_test, W2, Z2_test);\n",
        "    sumar_sesgo(Z2_test, b2);\n",
        "    memcpy(A2_test->datos, Z2_test->datos, X_test->filas * TAMAÑO_SALIDA * sizeof(float));\n",
        "    softmax(A2_test);\n",
        "\n",
        "    float acc_test = calcular_precision(A2_test, Y_test);\n",
        "    printf(\"\\n=== RESULTADO FINAL (TEST) ===\\n\");\n",
        "    printf(\"Precision: %.2f%%\\n\", acc_test * 100.0f);\n",
        "\n",
        "    // Liberar todo\n",
        "    free(indices);\n",
        "    liberar_matriz(X_train); liberar_matriz(Y_train);\n",
        "    liberar_matriz(X_test); liberar_matriz(Y_test);\n",
        "    liberar_matriz(W1); liberar_matriz(b1);\n",
        "    liberar_matriz(W2); liberar_matriz(b2);\n",
        "    liberar_matriz(X_batch); liberar_matriz(Y_batch);\n",
        "    liberar_matriz(Z1); liberar_matriz(A1);\n",
        "    liberar_matriz(Z2); liberar_matriz(A2);\n",
        "    liberar_matriz(dZ2); liberar_matriz(dW2); liberar_matriz(db2);\n",
        "    liberar_matriz(A1_T); liberar_matriz(dZ1); liberar_matriz(dW1);\n",
        "    liberar_matriz(db1); liberar_matriz(W2_T); liberar_matriz(X_batch_T);\n",
        "    liberar_matriz(Z1_test); liberar_matriz(A1_test);\n",
        "    liberar_matriz(Z2_test); liberar_matriz(A2_test);\n",
        "\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJCDyR-DI6s7"
      },
      "source": [
        "### Compilación y ejecución del archivo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_-kOiOWRI_py"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\"nvcc\" no se reconoce como un comando interno o externo,\n",
            "programa o archivo por lotes ejecutable.\n"
          ]
        }
      ],
      "source": [
        "!nvcc mainA.cu -o mlp_cuda -arch=sm_75 -lm -O3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiDN6EY3JlYM",
        "outputId": "dfb793da-30ce-4e41-cdb4-644e4c90bb3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== MLP en CUDA (Optimizado) ===\n",
            "  [GPU Mem] Usada: 102.94 MB | Libre: 14992.12 MB | Total: 15095.06 MB\n",
            "\n",
            "Cargando datos...\n",
            "Datos cargados correctamente. Train: 60000, Test: 10000\n",
            "\n",
            "Iniciando Entrenamiento...\n",
            "Epoca 1/10 | Tiempo: 1.82s | Loss: 1.8704 | Acc: 0.6969  [GPU Mem] Usada: 102.94 MB | Libre: 14992.12 MB | Total: 15095.06 MB\n",
            "Epoca 2/10 | Tiempo: 1.76s | Loss: 1.4101 | Acc: 0.7743  [GPU Mem] Usada: 102.94 MB | Libre: 14992.12 MB | Total: 15095.06 MB\n",
            "Epoca 3/10 | Tiempo: 1.50s | Loss: 1.0581 | Acc: 0.8091  [GPU Mem] Usada: 102.94 MB | Libre: 14992.12 MB | Total: 15095.06 MB\n",
            "Epoca 4/10 | Tiempo: 1.44s | Loss: 0.8473 | Acc: 0.8317  [GPU Mem] Usada: 102.94 MB | Libre: 14992.12 MB | Total: 15095.06 MB\n",
            "Epoca 5/10 | Tiempo: 1.45s | Loss: 0.7210 | Acc: 0.8462  [GPU Mem] Usada: 102.94 MB | Libre: 14992.12 MB | Total: 15095.06 MB\n",
            "Epoca 6/10 | Tiempo: 1.44s | Loss: 0.6385 | Acc: 0.8531  [GPU Mem] Usada: 102.94 MB | Libre: 14992.12 MB | Total: 15095.06 MB\n",
            "Epoca 7/10 | Tiempo: 1.46s | Loss: 0.5807 | Acc: 0.8630  [GPU Mem] Usada: 102.94 MB | Libre: 14992.12 MB | Total: 15095.06 MB\n",
            "Epoca 8/10 | Tiempo: 1.69s | Loss: 0.5388 | Acc: 0.8680  [GPU Mem] Usada: 102.94 MB | Libre: 14992.12 MB | Total: 15095.06 MB\n",
            "Epoca 9/10 | Tiempo: 1.66s | Loss: 0.5068 | Acc: 0.8731  [GPU Mem] Usada: 102.94 MB | Libre: 14992.12 MB | Total: 15095.06 MB\n",
            "Epoca 10/10 | Tiempo: 1.46s | Loss: 0.4809 | Acc: 0.8784  [GPU Mem] Usada: 102.94 MB | Libre: 14992.12 MB | Total: 15095.06 MB\n",
            "\n",
            "Entrenamiento finalizado en 20.89 segundos.\n",
            "\n",
            "=== RESULTADO FINAL (TEST) ===\n",
            "Precision: 88.72%\n"
          ]
        }
      ],
      "source": [
        "!./mlp_cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIx_mgDFAPqB",
        "outputId": "0a08871b-4a80-4c0d-a088-64b192c79efc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
